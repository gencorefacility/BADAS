{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Computing with Python \n",
    "A gentle introduction to some elements of scientific programming in Python.\n",
    "\n",
    "Programming Bootcamp for biology graduate students, September 2013.\n",
    "\n",
    "Mickey Atwal, Cold Spring Harbor Laboratory.\n",
    "([Lab website](http://atwallab.cshl.edu))\n",
    "\n",
    "---\n",
    "INTERNET RESOURCES\n",
    "\n",
    "* [NumPy reference](http://docs.scipy.org/doc/numpy/reference)\n",
    "* [SciPy reference](http://docs.scipy.org/doc/scipy/reference)\n",
    "* [Numpy for Matlab users](http://mathesaurus.sourceforge.net/matlab-numpy.html)\n",
    "* [Matplotlib gallery and code](http://matplotlib.org/gallery.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, dividing an integer by an integer gives you just an integer, removing the remainder. This is a legacy from older programming languages like C. In the future, this feature will change in Python, giving you a real result. If you want it now, you can import the new feature from the future to indicate that '/' means true division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the standard ways of loading some useful Python packages for scientific computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NumPy package and abbreviate the name to 'np'\n",
    "import numpy as np \n",
    "\n",
    "# load SciPy package and abbreviate the name to 'sp'\n",
    "import scipy as sp\n",
    "\n",
    "# load Matplotlib plotting package and abbreviate the name to 'plt'\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the text file containing the nucleotide counts of the E. Coli DNA binding sites of the transcription factor CRP (cAMP receptor protein) also known as CAP (catabolite gene activator protein). You can find a copy at http://atwallab.cshl.edu/links/crp_counts_matrix.txt. I will go through the steps of how to download a text file from the web, save it, and open it again in a numerical array format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the module that deals with URL stuff\n",
    "import urllib\n",
    "\n",
    "# web URL address of the file \n",
    "url=\"http://atwallab.cshl.edu/links/crp_counts_matrix.txt\"\n",
    "\n",
    "# name of the file to be saved into\n",
    "filename=\"data.txt\"\n",
    "\n",
    "# downloads the file and saves it into the local directory\n",
    "urllib.urlretrieve(url,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small tab-delimited text file where the counts data at each of the 42 nucleotide positions is stored as a series of strings. Let's take a look using the Unix command \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert this to a numerical array of numbers where we can perform computations.  The [genfromtxt](http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html) function in NumPy automatically generates a NumPy array from a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads data from text file and store in an integer NumPy array called 'counts'\n",
    "counts=np.genfromtxt(filename,dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Working with numerical arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.ndim # what is the dimensionality of the array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.shape # what is the size of the array? (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.dtype # what is the data type of the array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practise some array indexing to remember how they work in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the first five rows\n",
    "counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first row\n",
    "counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows 2 to 3, i.e. the second to third rows\n",
    "counts[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second column\n",
    "counts[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last two rows\n",
    "counts[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every third row beginning with the first\n",
    "counts[::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows 3 to 4, and columns 2 to 4 \n",
    "counts[2:4,1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computations on arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the minimum and maximum element of the array\n",
    "np.min(counts), np.max(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the elements greater than 200\n",
    "counts[counts>200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the indices of the elements greater than 200? \n",
    "# The Numpy function \"where\" returns the indices in separate arrays of rows and columns.\n",
    "np.where(counts>200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select elements that are greater than 200 and also divisible by 3, i.e. counts mod 3 = 0\n",
    "counts[(counts>200) & (counts%3==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot Product. Frequently when performing operations on arrays we have to take the dot product of two list of numbers, or two vectors, e.g. $\\vec{x}=\\{x_1,x_2,x_3\\}$ and $\\vec{y}=\\{y_1,y_2,y_3\\}$. The dot product $\\vec{x} \\cdot \\vec{y}$ is defined as\n",
    "$$\n",
    "x \\cdot y = \\sum_{i=1}^{3} x_i y_i\n",
    "$$\n",
    "NumPy provides an efficient way of doing this without explicitly writing a 'for loop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product between rows 3 and 8\n",
    "np.dot(counts[2],counts[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum each column of the array, i.e. sum along the rows, the dimension indexed as 0\n",
    "sum(counts,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum each row of the array, i.e. sum along the columns, the dimension indexed as 1\n",
    "sum(counts,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, median and standard deviation of each column\n",
    "np.mean(counts,0), np.median(counts,0), np.std(counts,0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add pseudocounts to each element. This is usually a good idea if your count data is undersampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 1 to EVERY element of the counts matrix to form a new matrix 'new_counts'\n",
    "new_counts=counts+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the probabilities of each nucleotide at each position, e.g. the probability of seeing an A at position i is\n",
    "$$\n",
    "\\begin{array}\n",
    "pp_i(A)&=&\\frac{\\rm{counts}_i(A)}{\\rm{counts}_i(A)+\\rm{counts}_i(T)+\\rm{counts}_i(G)+\\rm{counts}_i(C)} \\\\\n",
    "&=&\\frac{\\rm{counts}_i(A)}{\\rm{total\\_counts}_i}\n",
    "\\end{array}\n",
    "$$\n",
    "The total counts is the same for all positions, so we might just as well use only the first position to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts=sum(new_counts[0])\n",
    "prob=new_counts/total_counts\n",
    "print prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often a good idea to represent the data graphically to glean what's going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of the figure\n",
    "plt.figure(figsize=[15,2])\n",
    "\n",
    "# show the array flipped (transposed) and with no colour interpolation smoothing\n",
    "plt.imshow(prob.T,interpolation='nearest')\n",
    "\n",
    "# set the ticks\n",
    "plt.xticks(range(0,42),range(1,43))\n",
    "plt.yticks(range(4),['A','C','G','T'])\n",
    "\n",
    "# set the colorbar\n",
    "plt.clim([0,1])\n",
    "plt.colorbar(ticks=arange(0,1,0.2))\n",
    "\n",
    "# title\n",
    "plt.title('base frequency matrix of CRP binding sites',fontsize=15)\n",
    "\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know how conserved the individual positions are. We start by calculating the entropy. The entropy H of nucleotide variability at each site i is defined by, \n",
    "$$\n",
    "H_i=-\\sum_{n=\\{A,C,T,G\\}} p_i(n) \\log_2 p_i(n)\n",
    "$$\n",
    "From the formula, we see that this calculation involves a dot product between the rows of the probability array and the rows of a log probability array. We have to be remember that $0 \\log_2 0 = 0$, if any of the probabilities are zero. This is important since $\\log_20=-\\infty$ and computers don't like dealing with infinities. A simple way to deal with this is to remove all the zero probabilities in the above entropy summation formula.\n",
    "\n",
    "Let's use this opportunity to go over a simple example of how to define a new function in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a new function 'entropy_calc' which takes as input a 1D array p\n",
    "def entropy_calc(p):\n",
    "    p=p[p!=0] # modify p to include only those elements that are not equal to 0\n",
    "    return np.dot(-p,log2(p)) # the function returns the entropy result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plug in a few probability distributions into our entropy function to make sure it spits out the right answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic probability distributions\n",
    "dist1=np.array([1/4,1/4,1/4,1/4])\n",
    "dist2=np.array([1/8,1/8,1/4,1/2])\n",
    "dist3=np.array([0,0,1/2,1/2])\n",
    "dist4=np.array([0,0,0,1])\n",
    "\n",
    "# evaluate the entropies\n",
    "print entropy_calc(dist1)\n",
    "print entropy_calc(dist2)\n",
    "print entropy_calc(dist3)\n",
    "print entropy_calc(dist4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our DNA sequence probabilities, we can evaluate the entropy at each nucleotide position by the following simple command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through every row of the 'prob' array and evaluate the entropy \n",
    "DNA_entropy=np.array([entropy_calc(t) for t in prob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a new 1-dimensional NumPy array called 'DNA_entropy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print DNA_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most entropy values are fairly close to the maximum of two bits, representing a great deal of base variability. Let's plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nucleotide positions\n",
    "num_pos=len(DNA_entropy)\n",
    "\n",
    "# set the size of the figure\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "# plot a bar chart\n",
    "plt.bar(np.arange(num_pos),DNA_entropy,color='green')\n",
    "\n",
    "# axes labels\n",
    "plt.xlabel('position',fontsize=15)\n",
    "plt.ylabel('entropy (bits)',fontsize=15)\n",
    "\n",
    "# limit the x axis range to just to the total number of nucleotide positions\n",
    "plt.xlim(0,num_pos)\n",
    "\n",
    "# place the x axis ticks and labels to be centered at each bar\n",
    "plt.xticks(arange(num_pos)+0.5,arange(1,num_pos+1),fontsize=10)\n",
    "\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really want to calculate is the sequence conservation, R. This is defined to be the total possible entropy at each site minus the observed entropy:\n",
    "$$\n",
    "\\begin{array}\n",
    "RR &=& H_{max}-H_{obs} \\\\\n",
    "&=& \\log_2(\\rm{no. of  states})-\\left( - \\sum_{n=\\{A,C,T,G\\}} p(n) \\log_2 p(n) \\right) \\\\\n",
    "&=& 2 + \\sum_{n=\\{A,C,T,G\\}} p(n) \\log_2 p(n)\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conservation score at each position\n",
    "R=2-DNA_entropy\n",
    "\n",
    "# set the size of the figure\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "# plot a bar chart\n",
    "plt.bar(np.arange(num_pos),R,color='green')\n",
    "\n",
    "# axes labels\n",
    "plt.xlabel('position',fontsize=20)\n",
    "plt.ylabel('bits',fontsize=20)\n",
    "\n",
    "# limit the x axis range to just to the total number of nucleotide positions\n",
    "plt.xlim(0,num_pos)\n",
    "plt.ylim(0,2)\n",
    "\n",
    "# place the x axis ticks and labels to be centered at each bar\n",
    "plt.xticks(arange(num_pos)+0.5,arange(1,num_pos+1),fontsize=10,rotation=0)\n",
    "\n",
    "# set the title\n",
    "plt.title('Sequence Conservation of Binding Site', fontsize=20)\n",
    "\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random sampling, correlations and statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A frequent task in scientific programming is to analyze random samples from a known distribution.  A commonly used distribution is the gaussian (normal distribution)\n",
    "\n",
    "$$\n",
    "p(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x-\\mu)^2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the standard deviation and $\\mu$ is the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "N_gauss = 5000\n",
    "\n",
    "# standard deviations\n",
    "s = 2\n",
    "\n",
    "# mean\n",
    "u = 5\n",
    "\n",
    "# draw random samples from gaussian distribution\n",
    "samples=np.random.normal(u,s,N_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the data in two different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# plot histogram\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(samples,bins=20)\n",
    "plt.title('histogram')\n",
    "\n",
    "# plot boxplot\n",
    "plt.subplot(1,2,2)\n",
    "# the default is to produce a vertical boxplot. Setting vert=False gives a horizontal plot\n",
    "plt.boxplot(samples,vert=False) \n",
    "plt.title('boxplot')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw random samples from a two-dimensional Gaussian (normal) distribution centered around the origin, where the $x$ and $y$ variables are independent and therefore uncorrelated,\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "pp(x,y)&=&p(x)p(y) \\\\\n",
    "&=&\\displaystyle \\frac{1}{2 \\pi \\sigma_x \\sigma_y} \\exp{\\left( -\\frac{x^2}{2 \\sigma_x^2} - \\frac{y^2}{2 \\sigma_y^2} \\right)}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)) # set the figure size\n",
    "\n",
    "cm = plt.cm.RdBu # coloring scheme for contour plots\n",
    "\n",
    "N_2dgauss=100 # number of datapoints\n",
    "\n",
    "sx=7 # standard deviation in the x direction\n",
    "sy=2 # standard deviation in the y direction\n",
    "\n",
    "x=np.random.normal(0,sx,N_2dgauss) # random samples from gaussian in x-direction\n",
    "y=np.random.normal(0,sy,N_2dgauss) # random samples from gaussian in y-direction\n",
    "\n",
    "axs=max(abs(x))*1.2 # axes plotting range\n",
    "\n",
    "t=linspace(-axs,axs,1000) # range of mesh points for contour plot\n",
    "cx,cy=np.meshgrid(t,t) # mesh array\n",
    "z=(1/(2*pi*sx*sy))*exp(-((cx*cx)/(2*sx*sx))-((cy*cy)/(2*sy*sy))) # actual 2d gaussian\n",
    "\n",
    "plt.xlim([-axs,axs])\n",
    "plt.ylim([-axs,axs])\n",
    "\n",
    "# plots contour of 2d gaussian\n",
    "plt.subplot(1,2,1,aspect='equal') # setting axes scales to be equal\n",
    "contour(t,t,z,linspace(0.0000001,1/(2*pi*sx*sy),20),cmap=cm)\n",
    "plt.grid()\n",
    "plt.title('contour map of 2d gaussian distribution\\n $\\sigma_x=$ %d, $\\sigma_y=$ %d' % (sx,sy))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# plots the random samples on top of contour plot\n",
    "plt.subplot(1,2,2,aspect='equal')\n",
    "contour(t,t,z,linspace(0.0000001,1/(2*pi*sx*sy),20),cmap=cm)\n",
    "plt.plot(x,y,'bo',markersize=5)\n",
    "plt.grid()\n",
    "plt.title('%d random samples from the 2d gaussian\\n distribution' % N_2dgauss)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the covariance and correlation of the above data. As a reminder, the covariance matrix of two random variables $x$ and $y$ with associated distributions $p(x)$ and $p(y)$ is\n",
    "\n",
    "$$\n",
    "    {\\rm{covariance}}=\n",
    "    \\left(\n",
    "          \\begin{array}{cc}\n",
    "          \\langle (x - \\langle x \\rangle )^2 \\rangle\n",
    "          & \\langle (x - \\langle x \\rangle)(y - \\langle y \\rangle) \\rangle \\\\\n",
    "            \\langle (y - \\langle y \\rangle)(x - \\langle x \\rangle) \\rangle\n",
    "          & \\langle (y - \\langle y \\rangle )^2 \\rangle\n",
    "          \\end{array}\n",
    "          \\right)\n",
    "    $$\n",
    "    \n",
    "where the $\\left< \\right>$ brackets denotes the expectation or averaging with respect to the probability distributions (conventional physics notation). The covariance matrix is symmetric and the diagonal elements are the variances in the $x$ and $y$ directions. In practise, with real data, you don't have access to the underlying distributions $p(x)$ and $p(y)$ and so the means and variances have to be estimated from the finite number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance matrix of the sample\n",
    "np.cov(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal elements of the sample covariance matrix should be close to the variances (standard deviation squared) of the original gaussian distributions from which the data was sampled from. The more samples the better the agreement.\n",
    "\n",
    "The correlation matrix is a normalized covariance matrix. The off-diagonal element is the correlation coefficient between x and y, and ranges from -1 to 1,\n",
    "$$\n",
    "    {\\rm{correlation}}=\n",
    "    \\left(\n",
    "          \\begin{array}{cc}\n",
    "          1 & \n",
    "          \\frac{\\langle (x- \\langle x \\rangle)(y-\\langle y \\rangle) \\rangle  }\n",
    "{\\sqrt{\\langle ( x - \\langle x \\rangle)^2 \\rangle } \\sqrt{\\langle ( y - \\langle y \\rangle)^2 \\rangle } }  \\\\\n",
    "  \\frac{\\langle (y- \\langle y \\rangle)(x-\\langle x \\rangle) \\rangle  }\n",
    "{\\sqrt{\\langle ( y - \\langle y \\rangle)^2 \\rangle } \\sqrt{\\langle ( x - \\langle x \\rangle)^2 \\rangle } }       \n",
    " & 1\n",
    "          \\end{array}\n",
    "          \\right)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix of the sample \n",
    "np.corrcoef(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the correlation between $x$ and $y$ variables is not exactly equal to zero even though the variables are sampled from independent distributions. This illustrates an important point: when the number of samples is low, artificial correlations become likely due to statistical fluctutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another key thing to note is that the correlation between $x$ and $y$ variables, also known as Pearson correlation coefficient, is only sensitive to linear relationships. The following picture from [Wikipedia](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) shows the Pearson correlation coefficient for several sets of ($x$,$y$) points. Note that the coefficient does a bad job of detecting non-linear relationships (last row)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural spike rate exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine we have observed firing rates of a neuron when presented with two different stimuli, A and B. We will assume the observations are random samples from two gaussian distributions with differing means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "N=1000\n",
    "\n",
    "# standard deviation, the same for both stimuli\n",
    "s=2\n",
    "\n",
    "# means corresponding to differing stimuli\n",
    "u_A=8\n",
    "u_B=12\n",
    "\n",
    "# observed firing rates\n",
    "samples_A=np.random.normal(u_A,s,N)\n",
    "samples_B=np.random.normal(u_B,s,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=arange(0,20,1) # bin boundaries for both histograms\n",
    "plt.hist(samples_A,bins,alpha=0.3) # the alpha parameter adjusts the transparency of the colours\n",
    "plt.hist(samples_B,bins,alpha=0.3,)\n",
    "plt.legend(('A','B'))\n",
    "plt.xlabel('firing rates',fontsize=15)\n",
    "plt.title('histogram of neural firing rates',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are given a single firing rate, how do we decide which stimulus was presented? A sensible way would be to impose a threshold at rate=10. However, classification errors are unavoidable, when the neural responses do not separate out cleanly.\n",
    "\n",
    "Using this strategy, what fraction of the firing rates stemming from stimulus A would the neuron misclassify as deriving from stimulus B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold of our binary classifier\n",
    "threshold=10\n",
    "\n",
    "# misclassified fraction\n",
    "len(samples_A[samples_A > threshold])/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we label stimulus B as positive and stimulus A as negative then the above calculation represents the false positive rate in our binary classifier. The true positive rate is then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples_B[samples_B > threshold])/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we can carry out a better job at classifying if the distributions were more widely separated by having smaller variability or bigger differences in their means. \n",
    "\n",
    "Sometimes in data analysis we want to know if two sets of samples have the same mean or are significantly different. There is an example of a statistical test and there are many such tests implemented in SciPy. As an example, we can carry out a traditional t-test for the null hypothesis that the two independent samples from above have identical means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stats module from the SciPy package\n",
    "import scipy.stats as st\n",
    "\n",
    "# perform the two-sided t-test, asumming populations have identical variances\n",
    "t, pval = st.stats.ttest_ind(samples_A,samples_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-statistic\n",
    "print t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-value\n",
    "print pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statisticians love p-values and will make conclusions about hypotheses based on them. Do not get me started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A frequent task in science is to fit a curve to data, and guess the underyling generative model. Let's make up some fake noisy data of the form $y=x^3 + \\eta$, where $\\eta$ is noise drawn from a gaussian (normal) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of data points\n",
    "N=20\n",
    "\n",
    "# N equalled spaced x values, from -20 to 20\n",
    "x=np.linspace(-20,20,N)\n",
    "\n",
    "# noise term: N samples from a gaussian distribution with mean 0 and standard deviation 1000\n",
    "noise=np.random.normal(0,1000,N)\n",
    "\n",
    "# y values\n",
    "y=x**3+noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our fake noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'ro') # 'r' indicates red, 'o' indicates a small circle\n",
    "plt.title('noisy data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to fit polynomials of various orders using the Numpy \"polyfit\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight line fit. The \"fit1\" array consists of the coefficients of the best linear fit\n",
    "fit1=np.polyfit(x,y,1)\n",
    "\n",
    "# 3rd order polynomial fit\n",
    "fit3=np.polyfit(x,y,3)\n",
    "\n",
    "# 19th order polynomial fit\n",
    "fit19=np.polyfit(x,y,19)\n",
    "\n",
    "# create functions from the fits\n",
    "y_1=np.poly1d(fit1)\n",
    "y_3=np.poly1d(fit3)\n",
    "y_19=np.poly1d(fit19)\n",
    "\n",
    "print('linear fit: y_1=(%.2f)x+(%.2f)' % (fit1[0],fit1[1]))\n",
    "print('3rd order fit: y_3=(%.2f)x^3 + (%.2f)x^2 + (%.2f)x + (%.2f)' % (fit3[0],fit3[1],fit3[2],fit3[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'ro')\n",
    "plt.plot(x,y_1(x))\n",
    "plt.plot(x,y_3(x))\n",
    "plt.plot(x,y_19(x))\n",
    "\n",
    "# add a legend to the right of the plot\n",
    "legend_text=('data','linear fit: $y=mx+c$ ','3rd order polynomial','19th order polynomial')\n",
    "plt.legend(legend_text, loc='center left', bbox_to_anchor=(1,0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high order (19th) fit clearly tracks the data better. However, the 19th order polynomial in fact overfits the data since it will perform poorly on new data sampled from the original noisy curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
